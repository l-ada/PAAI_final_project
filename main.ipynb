{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'output_cleared' from 'IPython.display' (/usr/lib/python3/dist-packages/IPython/display.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43554/3062225342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutput_cleared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'output_cleared' from 'IPython.display' (/usr/lib/python3/dist-packages/IPython/display.py)"
     ]
    }
   ],
   "source": [
    "import torch as tch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "# device = 'cuda'\n",
    "# batch_size=\n",
    "device = tch.device(\"cuda\" if tch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input will be array in range[-1,1] \n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# should I augment the data?\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "subset_size = 500  # Choose the desired subset size\n",
    "train_subset = tch.utils.data.Subset(train_dataset, range(subset_size))\n",
    "test_subset = tch.utils.data.Subset(test_dataset, range(subset_size))\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = tch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = tch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.9999, 0.99970002, 0.9994001099940001, 0.9990003499500025, 0.9985008497750276, 0.9979017492651625, 0.9972032180406769, 0.9964054554662444, 0.9955086905563247, 0.9945131818657684, 0.9934192173657161, 0.9922271143048773, 0.990937219056281, 0.9895499069496022, 0.9880655820891778, 0.986484677157835, 0.9848076532066667, 0.9830349994308947, 0.9811672329319759, 0.979204898466112, 0.9771485681793332, 0.9749988413293387, 0.9727563439942812, 0.970421728768695, 0.9679956744467734, 0.9654788856932117, 0.96287209270184, 0.9601760508422749, 0.9573915402948322, 0.9545193656739477, 0.9515603556403585, 0.9485153625023093, 0.9453852618060516, 0.9421709519159112, 0.9388733535842055, 0.9354934095113023, 0.9320320838961105, 0.9284903619773053, 0.9248692495655938, 0.9211697725673315, 0.9173929764998054, 0.9135399259985063, 0.9096117043167128, 0.9056094128177193, 0.9015341704600397, 0.8973871132759235, 0.8931693938435266, 0.8888821807530776, 0.8845266580673875, 0.8801040247770505, 0.8756154942506875, 0.871062293680584, 0.8664456635240769, 0.8617668569410469, 0.8570271392278712, 0.8522277872481951, 0.8473700888608804, 0.8424553423454872, 0.8374848558256489, 0.832459946690695, 0.8273819410158818, 0.8222521729815834, 0.8170719842917994, 0.8118427235923319, 0.8065657458889818, 0.8012424119661145, 0.7958740878059415, 0.7904621440088611, 0.7850079552152, 0.7795128995286935, 0.7739783579420398, 0.7684057137648571, 0.7627963520543737, 0.7571516590491714, 0.7514730216063026, 0.7457618266420947, 0.7400194605769506, 0.7342473087844503, 0.7284467550450532, 0.7226191810046928, 0.7167659656385548, 0.7108884847203186, 0.70498811029714, 0.6990662101706441, 0.6931241473841936, 0.6871632797166894, 0.6811849591831542, 0.6751905315423424, 0.6691813358116155, 0.663158703789311, 0.6571239595848283, 0.6510784191566479, 0.6450233898584911, 0.6389601699938212, 0.63289004837888, 0.6268143039144427, 0.6207342051664726, 0.6146510099558411, 0.6085659649572782, 0.6024803053077055, 0.5963952542240977, 0.5903120226310119, 0.5842318087979125, 0.5781557979864143, 0.5720851621075569, 0.5660210593892168, 0.5599646340537521, 0.5539170160059715, 0.5478793205315065, 0.5418526480056599, 0.5358380836127971, 0.5298366970763337, 0.5238495423993712, 0.5178776576160183, 0.5119220645534341, 0.5059837686046142, 0.5000637585119403, 0.4941630061614994, 0.4882824663881775, 0.48242307679151936, 0.47658575756234195, 0.4707714113200814, 0.46498092296084437, 0.4592151595161299, 0.4534749700221783, 0.4477611853998989, 0.44207461834532014, 0.43641606323050003, 0.43078629601482654, 0.4251860741666338, 0.41961613659505087, 0.4140772035919962, 0.40856997678422263, 0.4030951390953141, 0.3976533547175274, 0.392245269093369, 0.38687150890678984, 0.38153268208387614, 0.37622937780291027, 0.37096216651366953, 0.36573159996582677, 0.360538211246312, 0.35538251482548977, 0.3502650066120027, 0.34518616401612867, 0.3401464460214932, 0.33514629326497725, 0.3301861281246556, 0.32526635481559824, 0.3203873594933643, 0.3155495103650145, 0.31075315780746626, 0.305998634493012, 0.30128625552181965, 0.29661631856123144, 0.29198910399167627, 0.2874048750590069, 0.2828638780330746, 0.2783663423723487, 0.27391248089439113, 0.26950248995199144, 0.2651365496147692, 0.26081482385604843, 0.25653746074480926, 0.2523045926425199, 0.2481163364046541, 0.24397279358669638, 0.23987405065443987, 0.23582017919837983, 0.23181123615200738, 0.22784726401380806, 0.22392829107277057, 0.22005433163721164, 0.21622538626672416, 0.2124414420070565, 0.2087024726277323, 0.20500843886222145, 0.2013592886504739, 0.19775495738363041, 0.19419536815072505, 0.19068043198719692, 0.18721004812502995, 0.1837841042443419, 0.180402476726246, 0.17706503090681047, 0.1737716213319438, 0.17052209201303645, 0.16731627668319135, 0.16415399905387904, 0.16103507307185533, 0.1579593031761829, 0.1549264845552002, 0.15193640340328482, 0.1489888371772611, 0.1460835548523045, 0.14322031717719935, 0.1403988769288085, 0.1376189791656181, 0.1348803614802223, 0.13218275425061787, 0.12952588089018044, 0.1269094580961988, 0.12433319609684597, 0.12179679889647031, 0.11929996451909268, 0.11684238524999938, 0.11442374787532439, 0.11204373391951764, 0.10970201988059972, 0.10739827746310712, 0.10513217380863556, 0.10290337172389248, 0.10071152990617357, 0.09855630316618147, 0.09643734264810858, 0.09435429604690944, 0.0923068078226915, 0.09029451941215683, 0.08831706943703059, 0.08637409390941592, 0.08446522643401783, 0.08259009840718264, 0.08074833921270247, 0.07893957641433794, 0.07716343594501533, 0.07541954229265799, 0.07370751868261465, 0.07202698725665103, 0.07037756924847373, 0.06875888515575883, 0.06717055490866079, 0.06561219803477986, 0.06408343382056948, 0.06258388146916816, 0.06111316025464271, 0.05967088967263314, 0.05825668958739173, 0.056870180375211805, 0.05551098306424424, 0.05417871947070238, 0.052873012331458455, 0.05159348543303716, 0.05033976373701436, 0.04911147350183121, 0.04790824240103635, 0.046729699637970856, 0.04557547605691298, 0.04444520425070153, 0.04333851866485906, 0.042255055698237586, 0.04119445380021182, 0.04015635356444648, 0.03914039781926599, 0.038146231714656635, 0.037173502805932894, 0.036221861134101016, 0.035290959302954616, 0.03438045255293839, 0.03348999883181728, 0.03261925886219003, 0.031767896205886874, 0.03093557732529264, 0.030121971641637442, 0.029326751590298215, 0.02854959267315531, 0.02779017350804938, 0.027048175875384464, 0.026323284761924158, 0.025615188401828397, 0.02492357831497903, 0.024248149342643098, 0.023588599680523206, 0.022944630909244923, 0.02231594802233161, 0.02170225945171749, 0.021103277090850088, 0.02051871631543354, 0.019948296001864485, 0.019391738543412466, 0.018848769864196917, 0.018319119431012984, 0.017802520263058418, 0.017298708939613865, 0.016807425605728833, 0.016328413975965562, 0.015861421336252946, 0.015406198543902487, 0.014962500025838094, 0.014530083775091372, 0.014108711345613723, 0.013698147845456364, 0.013298161928369037, 0.012908525783867825, 0.012529015125822112, 0.01215940917961036, 0.011799490667893894, 0.011449045795057447, 0.011107864230364735, 0.01077573908987683, 0.010452466917180525, 0.010137847662973392, 0.009831684663551596, 0.009533784618245982, 0.009243957565851304, 0.008962016860092839, 0.008687779144173999, 0.008421064324447858, 0.008161695543254863, 0.007909499150968287, 0.00766430467728827, 0.007425944801824604, 0.007194255324007677, 0.006969075132366237, 0.006750246173209937, 0.006537613418753824, 0.006331024834721204, 0.006130331347460542, 0.005935386810611297, 0.005746047971352797, 0.005562174436269507, 0.0053836286368652555, 0.0052102757947581945, 0.005041983886587505, 0.00487862360866207, 0.004720068341380552, 0.004566194113451547, 0.004416879565941681, 0.004272005916178794, 0.0041314569215365115, 0.003995118843125807, 0.0038628804094183425, 0.0037346327798256536, 0.0036102695082574593, 0.0034896865066816603, 0.0033727820087078248, 0.003259456533215242, 0.0031496128480458883, 0.0030431559337819373, 0.0029399929476267295, 0.0028400331874074204, 0.002743188055716827, 0.0026493710242113115, 0.0025584975980808638, 0.002470485280706882, 0.0023852535385224943, 0.0023027237660896162, 0.002222819251406307, 0.002145465141457367, 0.002070588408020505, 0.001998117813739787, 0.0019279838784775206, 0.001860118845955112, 0.0017944566506928965, 0.001730932885258368, 0.001669484767831696, 0.0016100511100968877, 0.0015525722854664288, 0.0014969901976467305, 0.0014432482495512129, 0.0013912913125673691, 0.001341065696183687, 0.0012925191179818376, 0.001245600673999097, 0.0012002608094655298, 0.001156451289920038, 0.0011141251727089646, 0.0010732367788705457, 0.0010337416654081096, 0.0009955965979545503, 0.0009587595238302319, 0.0009231895454961303, 0.0008888468944036743, 0.0008556929052424172, 0.0008236899905863507, 0.0007928016159393627, 0.0007629922751800427, 0.0007342274664057551, 0.0007064736681756175, 0.0006796983161517615, 0.0006538697801379945, 0.0006289573415147369, 0.000604931171068874, 0.0005817623072169361, 0.0005594226346198058, 0.0005378848631869432, 0.0005171225074679272, 0.0004971098664289184, 0.00047782200361147644, 0.00045923472767098997, 0.00044132457329182134, 0.00042406878247611113, 0.0004074452862030476, 0.0003914326864552678, 0.0003760102386089302, 0.0003611578341838775, 0.000346855983950196, 0.00033308580138737324, 0.0003198289864921558, 0.00030706780993111876, 0.000294785097533874, 0.0002829642151227656, 0.0002715890536748304, 0.0002606440148117347, 0.0002501139966133406, 0.00023998437975050033, 0.00023024101393263, 0.00022087020466557197, 0.00021185870031521664, 0.00020319367947232428, 0.00019486273861395897, 0.00018685388005692524, 0.00017915550019857992, 0.00017175637804037856, 0.0001646456639895069, 0.00015781286893394236, 0.00015124785358629038, 0.0001449408180917421, 0.00013888229189550725, 0.0001330631238650855, 0.0001274744726627519, 0.00012210779736365006, 0.00011695484831490402, 0.00011200765823118358, 0.0001072585335221814, 0.00010270004584748869, 9.832502389438567e-05, 9.412654537409541e-05, 9.009792923208413e-05, 8.623272806802771e-05, 8.252472076110252e-05, 7.8967905296299e-05, 7.555649178749889e-05, 7.228489569310019e-05, 6.914773122001964e-05, 6.613980491194879e-05, 6.325610941778782e-05, 6.04918174362305e-05, 5.784227583252361e-05, 5.530299992347582e-05, 5.286966792684288e-05, 5.053811557126911e-05, 4.830433086301901e-05, 4.616444900578727e-05, 4.4114747469930316e-05, 4.215164120751842e-05, 4.02716780096631e-05, 3.847153400263116e-05, 3.674800927931329e-05, 3.509802366267212e-05, 3.3518612597851874e-05, 3.2006923169688756e-05, 3.0560210242418824e-05, 2.917583271843725e-05, 2.7851249913020198e-05, 2.6584018041977778e-05, 2.537178681926359e-05, 2.4212296161623245e-05, 2.3103372997420902e-05, 2.2042928176839282e-05, 2.1028953480704675e-05, 2.0059518725244188e-05, 1.9132768960137907e-05, 1.8246921757283522e-05, 1.7400264587745566e-05, 1.6591152284415398e-05, 1.581800458796164e-05, 1.5079303773703833e-05, 1.4373592357094493e-05, 1.369947087554676e-05, 1.3055595744396063e-05, 1.2440677184835008e-05, 1.1853477221710796e-05, 1.1292807749123876e-05, 1.0757528661815403e-05, 1.0246546050379172e-05, 9.758810458381123e-06, 9.293315199516345e-06, 8.849094732979464e-06, 8.425223095269748e-06, 8.020812386696799e-06, 7.635011310896683e-06, 7.267003765711463e-06, 6.916007483827599e-06, 6.581272721610344e-06, 6.262080994612242e-06, 5.957743858274087e-06, 5.66760173237614e-06, 5.391022767836185e-06, 5.127401754488996e-06, 4.876159068519035e-06, 4.63673965825475e-06, 4.408612067068616e-06, 4.191267492162133e-06, 3.9842188780493236e-06, 3.7870000435858823e-06, 3.5991648414240226e-06, 3.420286348805249e-06, 3.2499560886347476e-06, 3.0877832798118734e-06, 2.9333941158212794e-06, 2.786431070618633e-06, 2.6465522308735776e-06, 2.5134306536606366e-06, 2.3867537487161406e-06, 2.2662226844059754e-06, 2.1515518165750333e-06, 2.042468139474679e-06, 1.9387107579893655e-06, 1.8400303804077069e-06, 1.7461888310069136e-06, 1.6569585817424603e-06, 1.5721223023572463e-06, 1.4914724282463195e-06, 1.4148107454344586e-06, 1.341947992044584e-06, 1.2727034756550835e-06, 1.2069047059637158e-06, 1.1443870421947954e-06, 1.0849933547048854e-06, 1.0285737002602313e-06, 9.749850104766731e-07, 9.240907929297907e-07, 8.757608444595627e-07, 8.298709762098816e-07, 7.863027499588628e-07, 7.449432253110266e-07, 7.056847173371355e-07, 6.684245642617349e-07, 6.330649048122891e-07, 5.995124648572377e-07, 5.676783529733184e-07, 5.374778645951378e-07, 5.088302944122169e-07, 4.816587566906046e-07, 4.5589001320765726e-07, 4.3145430849972686e-07, 4.0828521213329155e-07, 3.863194677205205e-07, 3.6549684841038443e-07, 3.4576001859622364e-07, 3.270544015901679e-07, 3.093280530239808e-07, 2.925315397447786e-07, 2.7661782398266266e-07, 2.6154215257560756e-07, 2.472619510449794e-07, 2.3373672232281902e-07, 2.2092794993952855e-07, 2.0879900548784845e-07, 1.9731506018601677e-07, 1.8644300036976723e-07, 1.7615134674935608e-07, 1.664101772741167e-07, 1.5719105345313063e-07, 1.484669499864819e-07, 1.402121875672335e-07, 1.324023687197386e-07, 1.2501431654517718e-07, 1.1802601625030178e-07, 1.1141655934028487e-07, 1.0516609036129489e-07, 9.925575608299012e-08, 9.366765701551778e-08, 8.838480115984257e-08, 8.339105989431147e-08, 7.867112590429344e-08, 7.421047306552e-08, 6.999531819539847e-08, 6.60125845900803e-08, 6.224986726844572e-08, 5.869539984741746e-08, 5.533802297614518e-08, 5.216715425961206e-08, 4.917275960511033e-08, 4.634532592781649e-08, 4.367583515437426e-08, 4.115573946596687e-08, 3.8776937724833986e-08, 3.65317530305661e-08, 3.441291135479326e-08, 3.241352120507977e-08, 3.0527054270944125e-08, 2.8747327006948083e-08, 2.7068483109742316e-08, 2.548497684782239e-08, 2.399155720454e-08, 2.2583252796633503e-08, 2.1255357532191455e-08, 2.000341697354538e-08, 1.88232153721062e-08, 1.7710763343614724e-08, 1.666228615367273e-08, 1.5674212584759935e-08, 1.4743164357225195e-08, 1.3865946077970295e-08, 1.3039535691723266e-08, 1.2261075410927387e-08, 1.152786310135393e-08, 1.083734410158283e-08, 1.018710345548786e-08, 9.574858537813039e-09, 8.998452053836694e-09, 8.455845394990341e-09, 7.945112333132925e-09, 7.464433036978384e-09, 7.012088394937494e-09, 6.586454629364788e-09, 6.185998187899409e-09, 5.809270898256335e-09, 5.454905373462698e-09, 5.1216106551441265e-09, 4.8081680830493055e-09, 4.513427379558383e-09, 4.236302938453498e-09, 3.975770307738608e-09, 3.73086285678191e-09, 3.500668618518466e-09, 3.284327297894025e-09, 3.081027438154385e-09, 2.890003736988813e-09, 2.7105345049218077e-09, 2.541939258715671e-09, 2.383576442897685e-09, 2.2348412728608693e-09, 2.095163693307065e-09, 1.9640064461060425e-09, 1.8408632419351937e-09, 1.7252570303416635e-09, 1.6167383631331729e-09, 1.5148838462557831e-09, 1.419294675557043e-09, 1.3295952520618378e-09, 1.2454318726063234e-09, 1.1664714918830825e-09, 1.0924005521485068e-09, 1.0229238770318617e-09, 9.57763626064932e-10, 8.966583067219894e-10, 8.393618409224544e-10, 7.856426831034172e-10, 7.352829871164881e-10, 6.880778193436096e-10, 6.438344155598155e-10, 6.023714791977634e-10, 5.635185187895077e-10, 5.271152224757055e-10, 4.930108675815274e-10, 4.610637633622444e-10, 4.3114072512003476e-10, 4.0311657798723255e-10, 3.768736887602637e-10, 3.5230152425309446e-10, 3.2929623471936737e-10, 3.0776026096872073e-10, 2.8760196387526953e-10, 2.6873527504505186e-10, 2.5107936747459196e-10, 2.345583450947638e-10, 2.191009501530189e-10, 2.0464028744291964e-10, 1.9111356444294264e-10, 1.7846184647681983e-10, 1.6662982605540666e-10, 1.5556560560532767e-10, 1.4522049283257338e-10, 1.35548808009924e-10, 1.2650770251566206e-10, 1.1805698798761584e-10, 1.1015897549124434e-10, 1.0277832413333098e-10, 9.588189858398447e-11, 8.943863499914071e-11, 8.341941486369854e-11, 7.779694630188525e-11, 7.254565242650799e-11, 6.764156632247605e-11, 6.306223228244442e-11, 5.878661293369469e-11, 5.479500191549682e-11, 5.1068941785243034e-11, 4.759114684966798e-11, 4.434543063452062e-11, 4.131663772218286e-11, 3.849057970198555e-11, 3.585397499239954e-11, 3.339439230792093e-11, 3.1100197556366764e-11, 2.8960503964488732e-11, 2.6965125241335458e-11, 2.5104531599683314e-11, 2.3369808466145196e-11, 2.1752617720287947e-11, 2.0245161312271993e-11, 1.8840147117200317e-11, 1.7530756892554894e-11, 1.6310616212833074e-11, 1.517376626279861e-11, 1.4114637377655267e-11, 1.3128024224957164e-11, 1.2209062529210162e-11, 1.1353207245912529e-11, 1.0556212097249469e-11, 9.81411038681283e-12, 9.123197015581207e-12, 8.480011625982733e-12, 7.881322805188352e-12, 7.324113282861535e-12, 6.805566062434938e-12, 6.323051428608301e-12, 5.874114777177112e-12, 5.45646521651982e-12, 5.067964893103608e-12, 4.706618996225321e-12, 4.370566399894833e-12, 4.058070902302353e-12, 3.767513025697504e-12, 3.497382341754993e-12, 3.246270289616985e-12, 3.012863455793524e-12, 2.7959372869763898e-12, 2.594350208585392e-12, 2.4070381235255263e-12, 2.233009267194631e-12, 2.0713393962497396e-12, 1.9211672900216335e-12, 1.781690544766063e-12, 1.6521616421615701e-12, 1.5318842746122078e-12, 1.420209910992978e-12, 1.3165345874904905e-12, 1.2202959091449357e-12, 1.1309702485955264e-12, 1.0480701293734742e-12, 9.711417818774612e-13, 8.997628609094678e-13, 8.33540314346531e-13, 7.721083931791917e-13, 7.151267937625673e-13, 6.622789237035136e-13, 6.132702833494536e-13, 5.67826955353259e-13, 5.256941952660472e-13, 4.866351165577799e-13, 4.5042946388588106e-13, 4.168724688263829e-13, 3.8577378265193477e-13, 3.5695648108783525e-13, 3.3025613630246516e-13, 3.0551995169341054e-13, 2.826059553164048e-13, 2.613822480721428e-13, 2.4172630301711766e-13, 2.235243123999287e-13, 2.0667057924497407e-13, 1.9106695051197852e-13, 1.7662228905327294e-13, 1.6325198177194018e-13, 1.5087748155362713e-13, 1.3942588070370684e-13, 1.2882951377022513e-13, 1.19025587772311e-13, 1.0995583798406089e-13, 1.0156620754587704e-13, 9.380654928937203e-14, 8.663034826873507e-14, 7.999446359134996e-14, 7.385888823389342e-14, 6.818652561753041e-14, 6.294298179754232e-14, 5.809637219913156e-14, 5.3617141902578524e-14, 4.9477898547699457e-14, 4.565325698996229e-14, 4.2119694898939206e-14, 3.8855418544271416e-14, 3.5840238065235954e-14, 3.305545156756712e-14, 3.0483737435610395e-14, 2.8109054289376347e-14, 2.5916548054804993e-14, 2.389246565172472e-14, 2.2024074837759845e-14, 2.029958977796325e-14, 1.8708101939370932e-14, 1.7239515937130313e-14, 1.588448998447187e-14, 1.4634380622693935e-14, 1.3481191429625654e-14, 1.2417525425828191e-14, 1.1436540917187764e-14, 1.0531910530638212e-14, 9.697783216611665e-15, 8.928749007534359e-15, 8.219806336336131e-15, 7.566331732597409e-15, 6.964051726682655e-15, 6.409016804066048e-15, 5.8975772631015774e-15, 5.426360839779762e-15, 4.992251972597381e-15, 4.5923725895923305e-15, 4.224064307907025e-15, 3.884871943982091e-15, 3.572528239685931e-15, 3.284939716391213e-15, 3.0201735752500812e-15, 2.7764455677273998e-15, 2.552108765855026e-15, 2.3456431666973544e-15, 2.155646070194869e-15, 1.9808231739020653e-15, 1.8199803321812173e-15, 1.6720159311748843e-15, 1.5359138343772486e-15, 1.4107368568755029e-15, 1.295620729354462e-15, 1.1897685157662023e-15, 1.092445451176527e-15, 1.0029741687251695e-15, 9.207302868897057e-16, 8.451383303360608e-16, 7.756679595824366e-16, 7.118304865088021e-16, 6.531756544204767e-16, 5.992886629307874e-16, 5.497874193727044e-16, 5.043199997905818e-16, 4.625623038079216e-16, 4.242158888222449e-16, 3.8900597004999857e-16, 3.566795739388437e-16, 3.270038333871319e-16, 2.997644140659838e-16, 2.7476406193288076e-16, 2.518212627614852e-16, 2.3076900519462503e-16, 2.1145363945983492e-16, 1.9373382447310074e-16, 1.774795565998076e-16, 1.6257127384542378e-16, 1.4889902971502364e-16, 1.3636173141301864e-16, 1.2486643745490116e-16, 1.143277101337075e-16, 1.0466701862740922e-16, 9.58121888515304e-17, 8.769689645580578e-17, 8.026019963635345e-17, 7.344610868722705e-17, 6.720318944881275e-17, 6.148419802671879e-17, 5.624574435484235e-17, 5.1447982361374295e-17, 4.705432466771293e-17, 4.303117990862347e-17, 3.9347710908445304e-17, 3.597561208359154e-17, 3.2888904566819384e-17, 3.00637476645296e-17, 2.7478265365380055e-17, 2.511238671742083e-17, 2.294769898237915e-17, 2.096731256019983e-17, 1.9155736754998562e-17, 1.7498765525691186e-17, 1.598337243116633e-17, 1.459761404138421e-17, 1.333054114259206e-17, 1.217211711730081e-17, 1.111314292809564e-17, 1.014518817905851e-17, 9.260527769844609e-18, 8.452083695537174e-18, 7.713371580547225e-18, 7.038451567249343e-18, 6.4218832099583e-18, 5.858684052444957e-18, 5.34429159264029e-18, 4.874528361647208e-18, 4.445569865822254e-18, 4.0539151606433135e-18, 3.696359843474573e-18, 3.369971269295768e-18, 3.072065809090022e-18, 2.800187984985555e-18, 2.5520913295158347e-18, 2.3257208285877803e-18, 2.1191968190091856e-18, 1.9308002217992692e-18, 1.7589590020591342e-18, 1.6022357549756653e-18, 1.459316325631836e-18, 1.328999377752913e-18, 1.2101868333818025e-18, 1.1018751117941311e-18, 1.0031471017773769e-18, 9.131648067479462e-19, 8.311626071019807e-19, 7.564410887235126e-19, 6.883613907383966e-19, 6.26340029432867e-19, 5.698441587780224e-19, 5.183872312403669e-19, 4.715250255362377e-19, 4.288520107252082e-19, 3.8999801855350434e-19, 3.546251982707015e-19, 3.224252302677218e-19, 2.9311677683638587e-19, 2.6644315014427476e-19, 2.4217017916613137e-19, 2.200842588261802e-19, 1.9999056599534992e-19, 1.8171142826337494e-19, 1.6508483257727612e-19, 1.4996306191319762e-19, 1.362114491357574e-19, 1.2370723810509487e-19, 1.1233854292323664e-19, 1.0200339697429888e-19, 9.260888411296596e-20, 8.40703449977505e-20, 7.631065215445812e-20, 6.925954789538619e-20, 6.285303971506296e-20, 5.703284823744813e-20, 5.1745903205836687e-20, 4.694388338833504e-20, 4.2582796621558714e-20, 3.8622596535753756e-20, 3.502683279827508e-20, 3.1762331981475844e-20, 2.879890640760415e-20, 2.610908854913392e-20, 2.3667888769789898e-20, 2.1452574380937562e-20, 1.9442468161443712e-20, 1.7618764647900293e-20, 1.5964362647462458e-20, 1.4463712558600987e-20, 1.3102677206836635e-20, 1.1868405013952623e-20, 1.0749214421136891e-20, 9.734488579781568e-21, 8.81457940899221e-21, 7.980720196901547e-21, 7.22494599425497e-21, 6.540021113999599e-21, 5.919373110281037e-21, 5.3570326648043386e-21, 4.847578858381446e-21, 4.386089351063533e-21, 3.968095035907178e-21, 3.589538769481633e-21, 3.246737816996137e-21, 2.936349681691306e-21, 2.655341017153448e-21, 2.4009593477101476e-21, 2.1707073462647447e-21, 1.9623194410233294e-21, 1.7737405427409873e-21, 1.6031067025293043e-21, 1.4487275270757322e-21, 1.3090701934656315e-21, 1.1827449197961981e-21, 1.0684917605438854e-21, 9.651686072992917e-22, 8.717402861127202e-22, 7.872686523883977e-22, 7.109035931067232e-22, 6.418748542160604e-22, 5.794846183862594e-22, 5.231007650172763e-22, 4.721507505045936e-22, 4.261160523303956e-22, 3.84527125622949e-22, 3.4695882544958687e-22, 3.1302625232061725e-22, 2.8238098221842883e-22, 2.547076459610228e-22, 2.2972082589224646e-22, 2.0716224078962784e-22, 1.867981925200074e-22, 1.6841725037603867e-22, 1.5182815121399886e-22, 1.3685789550429856e-22, 1.233500212180243e-22, 1.1116303912168348e-22, 1.0016901455254899e-22, 9.025228211184665e-23, 8.130828095456265e-23, 7.324249948387004e-23, 6.596951928512176e-23, 5.941214906818065e-23, 5.350064023589667e-23, 4.817197646840136e-23, 4.336923041450175e-23, 3.904098121913447e-23, 3.514078719534294e-23, 3.1626708475808646e-23]\n"
     ]
    }
   ],
   "source": [
    "n_directions = 6\n",
    "direction_embedding_dimension = 32\n",
    "time_embedding_dimension = 32\n",
    "T = 1000\n",
    "beta_1 = 10**-4\n",
    "beta_T = 10**-2\n",
    "beta_1_tensor = tch.tensor(beta_1)\n",
    "height = 28\n",
    "width = 28\n",
    "# list containing \\bar{alpha_t}\n",
    "diffusion_scheduler = [1]\n",
    "for i in range(1,T+1):\n",
    "    diffusion_scheduler.append(diffusion_scheduler[i-1]*(1-i*beta_1))\n",
    "print(diffusion_scheduler)\n",
    "diffusion_scheduler = tch.tensor(diffusion_scheduler).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeEmbedding(nn.Module):\n",
    "#     def __init__(self, embed_dim: int):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "\n",
    "#     def forward(self, t):\n",
    "#         # t: (batch_size,) - the timestep\n",
    "#         # Create the sinusoidal embedding\n",
    "#         half_dim = self.embed_dim // 2\n",
    "#         exponents = torch.arange(half_dim, dtype=torch.float32) / half_dim\n",
    "#         freqs = torch.pow(10000, -exponents).to(t.device)\n",
    "#         angles = t[:, None] * freqs  # Broadcasting over the batch dimension\n",
    "#         # Combine sine and cosine\n",
    "#         time_embedding = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "#         return time_embedding  # Shape: (batch_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define architecture and U-net\n",
    "# Unet is predicting the noise.\n",
    "#  I have image x. What image might I get if I denoise by time \"timestep\" in the past in given direction.\n",
    "\n",
    "\n",
    "# for now the same as direction embedding\n",
    "class TimeEmbedding(tch.nn.Module):\n",
    "    def __init__(self, time_embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.time_embedding = tch.nn.Linear(1, time_embedding_dimension)\n",
    "    def forward(self, timestep):\n",
    "        timestep = timestep.view(-1,1).float()\n",
    "        return self.time_embedding(timestep)\n",
    "\n",
    "class DirectionEmbedding(tch.nn.Module):\n",
    "    def __init__(self, n_classes, direction_embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.direction_embedding = tch.nn.Embedding(n_classes, direction_embedding_dimension)\n",
    "    def forward(self, class_label):\n",
    "         return self.direction_embedding(class_label)\n",
    "\n",
    "# a block in my modified UNet\n",
    "class Block(tch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_embedding_dimension, direction_embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.conv = tch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm = tch.nn.BatchNorm2d(out_channels)\n",
    "        self.activation = tch.nn.ReLU()\n",
    "        # rescale time_embedding and direction_embedding to match the dimension of the channels\n",
    "        self.time_embedding_projection = tch.nn.Linear(time_embedding_dimension, in_channels)\n",
    "        self.direction_embedding_projection = tch.nn.Linear(direction_embedding_dimension, in_channels)\n",
    "    def forward(self, x, time_embedding, direction_embedding):\n",
    "        \n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # why -1 instead of n_channels?\n",
    "        #  should be broadcastable to x.\n",
    "        time_embedding = self.time_embedding_projection(time_embedding).view(batch_size,-1,1,1)\n",
    "        direction_embedding = self.direction_embedding_projection(direction_embedding).view(batch_size,-1,1,1)\n",
    "\n",
    "        # adding time embedding to input\n",
    "        x = x+time_embedding+direction_embedding\n",
    "        \n",
    "        # forward pass\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 3\n",
    "feature2 = 4\n",
    "class modifiedUnet(tch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_embedding_dimension, direction_embedding_dimension, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_embedding = TimeEmbedding(time_embedding_dimension)\n",
    "        self.direction_embedding = DirectionEmbedding(n_classes, direction_embedding_dimension)\n",
    "        self.pool = tch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        self.block1 = Block(in_channels, feature1, time_embedding_dimension,direction_embedding_dimension)\n",
    "        # maxpool\n",
    "        self.block2 = Block(feature1, feature2, time_embedding_dimension,direction_embedding_dimension)\n",
    "        # maxpool\n",
    "        self.block3 = Block(feature2, feature2, time_embedding_dimension,direction_embedding_dimension)\n",
    "        \n",
    "        # upsample\n",
    "        self.up1 = tch.nn.ConvTranspose2d(feature2,feature2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.block4 = Block(feature2, feature1, time_embedding_dimension,direction_embedding_dimension)\n",
    "        \n",
    "        # upsample\n",
    "        self.up2 = tch.nn.ConvTranspose2d(feature1,feature1, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.block5 = Block(feature1, out_channels, time_embedding_dimension,direction_embedding_dimension)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, class_label, timestep):\n",
    "        time_embedding = self.time_embedding(timestep)\n",
    "        direction_embedding = self.direction_embedding(class_label)\n",
    "\n",
    "        # no skip connections for now\n",
    "        x1 = self.block1(x,time_embedding,direction_embedding)\n",
    "        # downsample\n",
    "        x1 = self.pool(x1)\n",
    "        x2 =  self.block2(x1,time_embedding,direction_embedding)\n",
    "        # downsample\n",
    "        x2 = self.pool(x2)\n",
    "        x3 =  self.block3(x2,time_embedding,direction_embedding)\n",
    "        # upsample\n",
    "        x3 = self.up1(x3)\n",
    "        # skip connection below\n",
    "        #  x3 = x3+x2\n",
    "        x4 = self.block4(x3,time_embedding,direction_embedding)\n",
    "\n",
    "\n",
    "        # upsample\n",
    "        x4 = self.up2(x4)\n",
    "        # skip connection below\n",
    "        # x4 = x4+x1\n",
    "        x5 = self.block5(x4, time_embedding, direction_embedding)\n",
    "        return x5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiedUnet = modifiedUnet(\n",
    "    in_channels=1, \n",
    "    out_channels=1,\n",
    "    time_embedding_dimension=16,\n",
    "    direction_embedding_dimension=16, \n",
    "    n_classes=10).to(device)\n",
    "#weights, loss function, optimizer\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "# optimizer and scheduler\n",
    "optimizer = tch.optim.AdamW(modifiedUnet.parameters(), lr=1e-4,\n",
    "                            weight_decay=0.1)\n",
    "scheduler = tch.optim.lr_scheduler.StepLR(optimizer, step_size=10000,\n",
    "                                          gamma=0.2)\n",
    "loss_func = tch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tch.randint(low=0,high=T, size=(batch_size,))\n",
    "a.shape\n",
    "noise = tch.randn(batch_size, 1, height, width, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "running_loss = 0.0\n",
    "epoch_loss_ = 0.0\n",
    "epoch_loss = 0.0\n",
    "n_epoch = 3\n",
    "\n",
    " \n",
    "for epoch in range(n_epoch):\n",
    "    i = 0\n",
    "    for data in train_loader:\n",
    "        ###### COMPLETER ICI ######\n",
    "        loss_val = 0 # requis aux lignes suivantes\n",
    "        images,labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        timesteps = tch.randint(1, T, size=(batch_size,), device=device)  # Move timesteps to the device\n",
    "        # generating a batch of random noise\n",
    "\n",
    "        noise = tch.randn(batch_size, 1, height, width, device=device)\n",
    "        alpha_bar = diffusion_scheduler[timesteps].view(-1,1,1,1).to(device)\n",
    "        noised_image = tch.sqrt(alpha_bar)*images+tch.sqrt(1-alpha_bar)*noise\n",
    "        predicted_noise = modifiedUnet(noised_image,labels, timesteps)\n",
    "        loss_val = loss_func(noise, predicted_noise)\n",
    "        ## Gradient calculation\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        #####\n",
    "\n",
    "        running_loss += loss_val.item()\n",
    "        epoch_loss += loss_val.item()\n",
    "        print(f\"Epoch = {epoch}\", end=\"\\r\", flush=True)\n",
    "    if epoch % 10 == 0:    # every 100 epoch...\n",
    "        # disp_loss(epoch_loss, epoch)\n",
    "        pass\n",
    "    i = i+1\n",
    "    epoch_loss = 0.0\n",
    "    scheduler.step()  \n",
    "\n",
    "#save the model weights after training\n",
    "tch.save(modifiedUnet.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "tensor([[[[ 0.2843, -0.1254,  0.3258,  ...,  0.4570, -0.1362, -0.1375],\n",
      "          [ 0.2731, -0.0479, -0.3370,  ...,  0.2387,  0.4787,  0.2098],\n",
      "          [ 0.3362, -0.2064, -0.2961,  ..., -0.0840, -0.0472, -0.2859],\n",
      "          ...,\n",
      "          [-0.3799,  0.2064, -0.0335,  ..., -0.2411, -0.2498, -0.1472],\n",
      "          [ 0.3017,  0.2287,  0.1740,  ..., -0.1616, -0.1287,  0.6310],\n",
      "          [-0.3150,  0.3799,  0.3195,  ...,  0.0925, -0.0479,  0.1493]]],\n",
      "\n",
      "\n",
      "        [[[-0.3271,  0.2176,  0.0902,  ..., -0.2250,  0.2685, -0.4917],\n",
      "          [-0.6474,  0.1665, -0.4795,  ..., -0.0912,  0.4784,  0.4119],\n",
      "          [ 0.2659,  0.2122,  0.3020,  ..., -0.3189, -0.2113, -0.3505],\n",
      "          ...,\n",
      "          [-0.0154, -0.2211,  0.2403,  ...,  0.1412, -0.2264, -0.1517],\n",
      "          [ 0.3163,  0.2225,  0.0846,  ...,  0.0016,  0.1408,  0.0258],\n",
      "          [-0.0987,  0.2383, -0.0798,  ..., -0.1580, -0.0022, -0.2956]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4175,  0.3033,  0.2268,  ..., -0.3193, -0.2854, -0.0948],\n",
      "          [-0.0125, -0.1745, -0.0377,  ...,  0.5500, -1.0000, -0.0831],\n",
      "          [-0.2054,  0.1070, -0.0287,  ...,  0.0272,  0.2957, -0.1305],\n",
      "          ...,\n",
      "          [ 0.3135, -0.1384,  0.5093,  ...,  0.0372,  0.1523, -0.0078],\n",
      "          [-0.5938, -0.1026,  0.0011,  ..., -0.1505, -0.3255, -0.2930],\n",
      "          [ 0.2693,  0.1990,  0.2167,  ..., -0.4800, -0.0322, -0.1909]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4980,  0.1314,  0.0593,  ...,  0.0445,  0.0841,  0.2324],\n",
      "          [ 0.5682, -0.0947, -0.0842,  ..., -0.1575, -0.1231,  0.3330],\n",
      "          [ 0.0041,  0.1294, -0.0311,  ..., -0.4137, -0.1202,  0.0330],\n",
      "          ...,\n",
      "          [-0.0442,  0.5055, -0.2993,  ...,  0.4209,  0.1708, -0.1416],\n",
      "          [ 0.5491, -0.3618, -0.5043,  ...,  0.1989,  0.3181, -0.0668],\n",
      "          [-0.2595,  0.0137,  0.1181,  ..., -0.1729,  0.4916,  0.0840]]],\n",
      "\n",
      "\n",
      "        [[[-0.3245,  0.1084, -0.1834,  ..., -0.2537, -0.2725,  0.1985],\n",
      "          [ 0.5324, -0.1453, -0.3734,  ...,  0.1861,  0.2274, -0.0938],\n",
      "          [ 0.4564,  0.3031,  0.5450,  ...,  0.1257,  0.2983,  0.1738],\n",
      "          ...,\n",
      "          [ 0.3487, -0.0595,  0.2458,  ..., -0.2219,  0.1565, -0.4116],\n",
      "          [ 0.0151,  0.4462, -0.1680,  ..., -0.0171, -0.1976, -0.0127],\n",
      "          [-0.3137,  0.3914, -0.1562,  ...,  0.3494,  0.1477, -0.0037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0478,  0.5394, -0.0903,  ..., -0.5625,  0.0417,  0.5676],\n",
      "          [-0.1994, -0.1703, -0.0367,  ...,  0.2709, -0.0570,  0.0492],\n",
      "          [-0.2577,  0.2229,  0.5839,  ..., -0.1192,  0.3426, -0.0796],\n",
      "          ...,\n",
      "          [-0.3311,  0.2026, -0.2114,  ..., -0.0135,  0.0775,  0.0309],\n",
      "          [-0.1317, -0.2629,  0.1256,  ...,  0.1734,  0.3617,  0.3148],\n",
      "          [-0.2698,  0.0571,  0.4171,  ..., -0.1431,  0.1920,  0.1456]]]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(images):\n",
    "    # for each image take a maximu of absolute values. Look at channels, height and width\n",
    "    # thus each image gets scaled individually\n",
    "    max_vals = tch.amax(tch.abs(images), dim=(1, 2, 3), keepdim=True)\n",
    "    # images are between [0,1]\n",
    "    images = images/max_vals\n",
    "    return images\n",
    "# how to generate image from noise\n",
    "# algorith from DDPM paper\n",
    "def generate_image(n_images,labels, noise_predictor):\n",
    "    noise_predictor = noise_predictor.cpu()\n",
    "    labels = tch.tensor(labels)\n",
    "    images = tch.randn(n_images,1,height,width)\n",
    "    # normalize images\n",
    "    images = normalize_data(images)\n",
    "    i = T\n",
    "    while(i>0):\n",
    "        times = tch.tensor([i] * n_images)\n",
    "        alpha_bar = diffusion_scheduler[i]\n",
    "        # sigma_t * z term \n",
    "        if i>1:\n",
    "            noise = tch.sqrt(i*beta_1_tensor)*tch.randn(n_images,1, height, width)\n",
    "        else:\n",
    "            noise = images*0\n",
    "        # i*beta = 1-alphta_t = beta_t\n",
    "        images = (1/tch.sqrt(alpha_bar))*(images-i*beta_1_tensor/tch.sqrt(1-alpha_bar)*noise_predictor(images,labels,times)) +noise \n",
    "        images = normalize_data(images)\n",
    "        i = i-1\n",
    "    return images.detach().numpy()\n",
    "    # return images.detach().numpy().squeeze(1)\n",
    "\n",
    "# img = generate_image(1, tch.tensor([1]),modifiedUnet).detach().numpy().squeeze(0)\n",
    "# plt.imshow(img[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
