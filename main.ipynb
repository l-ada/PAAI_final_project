{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "# device = 'cuda'\n",
    "# batch_size=\n",
    "device = tch.device(\"cuda\" if tch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input will be array in range[-1,1] \n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# should I augment the data?\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "subset_size = 500  # Choose the desired subset size\n",
    "train_subset = tch.utils.data.Subset(train_dataset, range(subset_size))\n",
    "test_subset = tch.utils.data.Subset(test_dataset, range(subset_size))\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = tch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = tch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.9999, 0.99970002, 0.9994001099940001, 0.9990003499500025, 0.9985008497750276, 0.9979017492651625, 0.9972032180406769, 0.9964054554662444, 0.9955086905563247, 0.9945131818657684, 0.9934192173657161, 0.9922271143048773, 0.990937219056281, 0.9895499069496022, 0.9880655820891778, 0.986484677157835, 0.9848076532066667, 0.9830349994308947, 0.9811672329319759, 0.979204898466112, 0.9771485681793332, 0.9749988413293387, 0.9727563439942812, 0.970421728768695, 0.9679956744467734, 0.9654788856932117, 0.96287209270184, 0.9601760508422749, 0.9573915402948322, 0.9545193656739477, 0.9515603556403585, 0.9485153625023093, 0.9453852618060516, 0.9421709519159112, 0.9388733535842055, 0.9354934095113023, 0.9320320838961105, 0.9284903619773053, 0.9248692495655938, 0.9211697725673315, 0.9173929764998054, 0.9135399259985063, 0.9096117043167128, 0.9056094128177193, 0.9015341704600397, 0.8973871132759235, 0.8931693938435266, 0.8888821807530776, 0.8845266580673875, 0.8801040247770505, 0.8756154942506875, 0.871062293680584, 0.8664456635240769, 0.8617668569410469, 0.8570271392278712, 0.8522277872481951, 0.8473700888608804, 0.8424553423454872, 0.8374848558256489, 0.832459946690695, 0.8273819410158818, 0.8222521729815834, 0.8170719842917994, 0.8118427235923319, 0.8065657458889818, 0.8012424119661145, 0.7958740878059415, 0.7904621440088611, 0.7850079552152, 0.7795128995286935, 0.7739783579420398, 0.7684057137648571, 0.7627963520543737, 0.7571516590491714, 0.7514730216063026, 0.7457618266420947, 0.7400194605769506, 0.7342473087844503, 0.7284467550450532, 0.7226191810046928, 0.7167659656385548, 0.7108884847203186, 0.70498811029714, 0.6990662101706441, 0.6931241473841936, 0.6871632797166894, 0.6811849591831542, 0.6751905315423424, 0.6691813358116155, 0.663158703789311, 0.6571239595848283, 0.6510784191566479, 0.6450233898584911, 0.6389601699938212, 0.63289004837888, 0.6268143039144427, 0.6207342051664726, 0.6146510099558411, 0.6085659649572782, 0.6024803053077055, 0.5963952542240977, 0.5903120226310119, 0.5842318087979125, 0.5781557979864143, 0.5720851621075569, 0.5660210593892168, 0.5599646340537521, 0.5539170160059715, 0.5478793205315065, 0.5418526480056599, 0.5358380836127971, 0.5298366970763337, 0.5238495423993712, 0.5178776576160183, 0.5119220645534341, 0.5059837686046142, 0.5000637585119403, 0.4941630061614994, 0.4882824663881775, 0.48242307679151936, 0.47658575756234195, 0.4707714113200814, 0.46498092296084437, 0.4592151595161299, 0.4534749700221783, 0.4477611853998989, 0.44207461834532014, 0.43641606323050003, 0.43078629601482654, 0.4251860741666338, 0.41961613659505087, 0.4140772035919962, 0.40856997678422263, 0.4030951390953141, 0.3976533547175274, 0.392245269093369, 0.38687150890678984, 0.38153268208387614, 0.37622937780291027, 0.37096216651366953, 0.36573159996582677, 0.360538211246312, 0.35538251482548977, 0.3502650066120027, 0.34518616401612867, 0.3401464460214932, 0.33514629326497725, 0.3301861281246556, 0.32526635481559824, 0.3203873594933643, 0.3155495103650145, 0.31075315780746626, 0.305998634493012, 0.30128625552181965, 0.29661631856123144, 0.29198910399167627, 0.2874048750590069, 0.2828638780330746, 0.2783663423723487, 0.27391248089439113, 0.26950248995199144, 0.2651365496147692, 0.26081482385604843, 0.25653746074480926, 0.2523045926425199, 0.2481163364046541, 0.24397279358669638, 0.23987405065443987, 0.23582017919837983, 0.23181123615200738, 0.22784726401380806, 0.22392829107277057, 0.22005433163721164, 0.21622538626672416, 0.2124414420070565, 0.2087024726277323, 0.20500843886222145, 0.2013592886504739, 0.19775495738363041, 0.19419536815072505, 0.19068043198719692, 0.18721004812502995, 0.1837841042443419, 0.180402476726246, 0.17706503090681047, 0.1737716213319438, 0.17052209201303645, 0.16731627668319135, 0.16415399905387904, 0.16103507307185533, 0.1579593031761829, 0.1549264845552002, 0.15193640340328482, 0.1489888371772611, 0.1460835548523045, 0.14322031717719935, 0.1403988769288085, 0.1376189791656181, 0.1348803614802223, 0.13218275425061787, 0.12952588089018044, 0.1269094580961988, 0.12433319609684597, 0.12179679889647031, 0.11929996451909268, 0.11684238524999938, 0.11442374787532439, 0.11204373391951764, 0.10970201988059972, 0.10739827746310712, 0.10513217380863556, 0.10290337172389248, 0.10071152990617357, 0.09855630316618147, 0.09643734264810858, 0.09435429604690944, 0.0923068078226915, 0.09029451941215683, 0.08831706943703059, 0.08637409390941592, 0.08446522643401783, 0.08259009840718264, 0.08074833921270247, 0.07893957641433794, 0.07716343594501533, 0.07541954229265799, 0.07370751868261465, 0.07202698725665103, 0.07037756924847373, 0.06875888515575883, 0.06717055490866079, 0.06561219803477986, 0.06408343382056948, 0.06258388146916816, 0.06111316025464271, 0.05967088967263314, 0.05825668958739173, 0.056870180375211805, 0.05551098306424424, 0.05417871947070238, 0.052873012331458455, 0.05159348543303716, 0.05033976373701436, 0.04911147350183121, 0.04790824240103635, 0.046729699637970856, 0.04557547605691298, 0.04444520425070153, 0.04333851866485906, 0.042255055698237586, 0.04119445380021182, 0.04015635356444648, 0.03914039781926599, 0.038146231714656635, 0.037173502805932894, 0.036221861134101016, 0.035290959302954616, 0.03438045255293839, 0.03348999883181728, 0.03261925886219003, 0.031767896205886874, 0.03093557732529264, 0.030121971641637442, 0.029326751590298215, 0.02854959267315531, 0.02779017350804938, 0.027048175875384464, 0.026323284761924158, 0.025615188401828397, 0.02492357831497903, 0.024248149342643098, 0.023588599680523206, 0.022944630909244923, 0.02231594802233161, 0.02170225945171749, 0.021103277090850088, 0.02051871631543354, 0.019948296001864485, 0.019391738543412466, 0.018848769864196917, 0.018319119431012984, 0.017802520263058418, 0.017298708939613865, 0.016807425605728833, 0.016328413975965562, 0.015861421336252946, 0.015406198543902487, 0.014962500025838094, 0.014530083775091372, 0.014108711345613723, 0.013698147845456364, 0.013298161928369037, 0.012908525783867825, 0.012529015125822112, 0.01215940917961036, 0.011799490667893894, 0.011449045795057447, 0.011107864230364735, 0.01077573908987683, 0.010452466917180525, 0.010137847662973392, 0.009831684663551596, 0.009533784618245982, 0.009243957565851304, 0.008962016860092839, 0.008687779144173999, 0.008421064324447858, 0.008161695543254863, 0.007909499150968287, 0.00766430467728827, 0.007425944801824604, 0.007194255324007677, 0.006969075132366237, 0.006750246173209937, 0.006537613418753824, 0.006331024834721204, 0.006130331347460542, 0.005935386810611297, 0.005746047971352797, 0.005562174436269507, 0.0053836286368652555, 0.0052102757947581945, 0.005041983886587505, 0.00487862360866207, 0.004720068341380552, 0.004566194113451547, 0.004416879565941681, 0.004272005916178794, 0.0041314569215365115, 0.003995118843125807, 0.0038628804094183425, 0.0037346327798256536, 0.0036102695082574593, 0.0034896865066816603, 0.0033727820087078248, 0.003259456533215242, 0.0031496128480458883, 0.0030431559337819373, 0.0029399929476267295, 0.0028400331874074204, 0.002743188055716827, 0.0026493710242113115, 0.0025584975980808638, 0.002470485280706882, 0.0023852535385224943, 0.0023027237660896162, 0.002222819251406307, 0.002145465141457367, 0.002070588408020505, 0.001998117813739787, 0.0019279838784775206, 0.001860118845955112, 0.0017944566506928965, 0.001730932885258368, 0.001669484767831696, 0.0016100511100968877, 0.0015525722854664288, 0.0014969901976467305, 0.0014432482495512129, 0.0013912913125673691, 0.001341065696183687, 0.0012925191179818376, 0.001245600673999097, 0.0012002608094655298, 0.001156451289920038, 0.0011141251727089646, 0.0010732367788705457, 0.0010337416654081096, 0.0009955965979545503, 0.0009587595238302319, 0.0009231895454961303, 0.0008888468944036743, 0.0008556929052424172, 0.0008236899905863507, 0.0007928016159393627, 0.0007629922751800427, 0.0007342274664057551, 0.0007064736681756175, 0.0006796983161517615, 0.0006538697801379945, 0.0006289573415147369, 0.000604931171068874, 0.0005817623072169361, 0.0005594226346198058, 0.0005378848631869432, 0.0005171225074679272, 0.0004971098664289184, 0.00047782200361147644, 0.00045923472767098997, 0.00044132457329182134, 0.00042406878247611113, 0.0004074452862030476, 0.0003914326864552678, 0.0003760102386089302, 0.0003611578341838775, 0.000346855983950196, 0.00033308580138737324, 0.0003198289864921558, 0.00030706780993111876, 0.000294785097533874, 0.0002829642151227656, 0.0002715890536748304, 0.0002606440148117347, 0.0002501139966133406, 0.00023998437975050033, 0.00023024101393263, 0.00022087020466557197, 0.00021185870031521664, 0.00020319367947232428, 0.00019486273861395897, 0.00018685388005692524, 0.00017915550019857992, 0.00017175637804037856, 0.0001646456639895069, 0.00015781286893394236, 0.00015124785358629038, 0.0001449408180917421, 0.00013888229189550725, 0.0001330631238650855, 0.0001274744726627519, 0.00012210779736365006, 0.00011695484831490402, 0.00011200765823118358, 0.0001072585335221814, 0.00010270004584748869, 9.832502389438567e-05, 9.412654537409541e-05, 9.009792923208413e-05, 8.623272806802771e-05, 8.252472076110252e-05, 7.8967905296299e-05, 7.555649178749889e-05, 7.228489569310019e-05, 6.914773122001964e-05, 6.613980491194879e-05, 6.325610941778782e-05, 6.04918174362305e-05, 5.784227583252361e-05, 5.530299992347582e-05, 5.286966792684288e-05, 5.053811557126911e-05, 4.830433086301901e-05, 4.616444900578727e-05, 4.4114747469930316e-05, 4.215164120751842e-05, 4.02716780096631e-05, 3.847153400263116e-05, 3.674800927931329e-05, 3.509802366267212e-05, 3.3518612597851874e-05, 3.2006923169688756e-05, 3.0560210242418824e-05, 2.917583271843725e-05, 2.7851249913020198e-05, 2.6584018041977778e-05, 2.537178681926359e-05, 2.4212296161623245e-05, 2.3103372997420902e-05, 2.2042928176839282e-05, 2.1028953480704675e-05, 2.0059518725244188e-05, 1.9132768960137907e-05, 1.8246921757283522e-05, 1.7400264587745566e-05, 1.6591152284415398e-05, 1.581800458796164e-05, 1.5079303773703833e-05, 1.4373592357094493e-05, 1.369947087554676e-05, 1.3055595744396063e-05, 1.2440677184835008e-05, 1.1853477221710796e-05, 1.1292807749123876e-05, 1.0757528661815403e-05, 1.0246546050379172e-05, 9.758810458381123e-06, 9.293315199516345e-06, 8.849094732979464e-06, 8.425223095269748e-06, 8.020812386696799e-06, 7.635011310896683e-06, 7.267003765711463e-06, 6.916007483827599e-06, 6.581272721610344e-06, 6.262080994612242e-06, 5.957743858274087e-06, 5.66760173237614e-06, 5.391022767836185e-06, 5.127401754488996e-06, 4.876159068519035e-06, 4.63673965825475e-06, 4.408612067068616e-06, 4.191267492162133e-06, 3.9842188780493236e-06, 3.7870000435858823e-06, 3.5991648414240226e-06, 3.420286348805249e-06, 3.2499560886347476e-06, 3.0877832798118734e-06, 2.9333941158212794e-06, 2.786431070618633e-06, 2.6465522308735776e-06, 2.5134306536606366e-06, 2.3867537487161406e-06, 2.2662226844059754e-06, 2.1515518165750333e-06, 2.042468139474679e-06, 1.9387107579893655e-06, 1.8400303804077069e-06, 1.7461888310069136e-06, 1.6569585817424603e-06, 1.5721223023572463e-06, 1.4914724282463195e-06, 1.4148107454344586e-06, 1.341947992044584e-06, 1.2727034756550835e-06, 1.2069047059637158e-06, 1.1443870421947954e-06, 1.0849933547048854e-06, 1.0285737002602313e-06, 9.749850104766731e-07, 9.240907929297907e-07, 8.757608444595627e-07, 8.298709762098816e-07, 7.863027499588628e-07, 7.449432253110266e-07, 7.056847173371355e-07, 6.684245642617349e-07, 6.330649048122891e-07, 5.995124648572377e-07, 5.676783529733184e-07, 5.374778645951378e-07, 5.088302944122169e-07, 4.816587566906046e-07, 4.5589001320765726e-07, 4.3145430849972686e-07, 4.0828521213329155e-07, 3.863194677205205e-07, 3.6549684841038443e-07, 3.4576001859622364e-07, 3.270544015901679e-07, 3.093280530239808e-07, 2.925315397447786e-07, 2.7661782398266266e-07, 2.6154215257560756e-07, 2.472619510449794e-07, 2.3373672232281902e-07, 2.2092794993952855e-07, 2.0879900548784845e-07, 1.9731506018601677e-07, 1.8644300036976723e-07, 1.7615134674935608e-07, 1.664101772741167e-07, 1.5719105345313063e-07, 1.484669499864819e-07, 1.402121875672335e-07, 1.324023687197386e-07, 1.2501431654517718e-07, 1.1802601625030178e-07, 1.1141655934028487e-07, 1.0516609036129489e-07, 9.925575608299012e-08, 9.366765701551778e-08, 8.838480115984257e-08, 8.339105989431147e-08, 7.867112590429344e-08, 7.421047306552e-08, 6.999531819539847e-08, 6.60125845900803e-08, 6.224986726844572e-08, 5.869539984741746e-08, 5.533802297614518e-08, 5.216715425961206e-08, 4.917275960511033e-08, 4.634532592781649e-08, 4.367583515437426e-08, 4.115573946596687e-08, 3.8776937724833986e-08, 3.65317530305661e-08, 3.441291135479326e-08, 3.241352120507977e-08, 3.0527054270944125e-08, 2.8747327006948083e-08, 2.7068483109742316e-08, 2.548497684782239e-08, 2.399155720454e-08, 2.2583252796633503e-08, 2.1255357532191455e-08, 2.000341697354538e-08, 1.88232153721062e-08, 1.7710763343614724e-08, 1.666228615367273e-08, 1.5674212584759935e-08, 1.4743164357225195e-08, 1.3865946077970295e-08, 1.3039535691723266e-08, 1.2261075410927387e-08, 1.152786310135393e-08, 1.083734410158283e-08, 1.018710345548786e-08, 9.574858537813039e-09, 8.998452053836694e-09, 8.455845394990341e-09, 7.945112333132925e-09, 7.464433036978384e-09, 7.012088394937494e-09, 6.586454629364788e-09, 6.185998187899409e-09, 5.809270898256335e-09, 5.454905373462698e-09, 5.1216106551441265e-09, 4.8081680830493055e-09, 4.513427379558383e-09, 4.236302938453498e-09, 3.975770307738608e-09, 3.73086285678191e-09, 3.500668618518466e-09, 3.284327297894025e-09, 3.081027438154385e-09, 2.890003736988813e-09, 2.7105345049218077e-09, 2.541939258715671e-09, 2.383576442897685e-09, 2.2348412728608693e-09, 2.095163693307065e-09, 1.9640064461060425e-09, 1.8408632419351937e-09, 1.7252570303416635e-09, 1.6167383631331729e-09, 1.5148838462557831e-09, 1.419294675557043e-09, 1.3295952520618378e-09, 1.2454318726063234e-09, 1.1664714918830825e-09, 1.0924005521485068e-09, 1.0229238770318617e-09, 9.57763626064932e-10, 8.966583067219894e-10, 8.393618409224544e-10, 7.856426831034172e-10, 7.352829871164881e-10, 6.880778193436096e-10, 6.438344155598155e-10, 6.023714791977634e-10, 5.635185187895077e-10, 5.271152224757055e-10, 4.930108675815274e-10, 4.610637633622444e-10, 4.3114072512003476e-10, 4.0311657798723255e-10, 3.768736887602637e-10, 3.5230152425309446e-10, 3.2929623471936737e-10, 3.0776026096872073e-10, 2.8760196387526953e-10, 2.6873527504505186e-10, 2.5107936747459196e-10, 2.345583450947638e-10, 2.191009501530189e-10, 2.0464028744291964e-10, 1.9111356444294264e-10, 1.7846184647681983e-10, 1.6662982605540666e-10, 1.5556560560532767e-10, 1.4522049283257338e-10, 1.35548808009924e-10, 1.2650770251566206e-10, 1.1805698798761584e-10, 1.1015897549124434e-10, 1.0277832413333098e-10, 9.588189858398447e-11, 8.943863499914071e-11, 8.341941486369854e-11, 7.779694630188525e-11, 7.254565242650799e-11, 6.764156632247605e-11, 6.306223228244442e-11, 5.878661293369469e-11, 5.479500191549682e-11, 5.1068941785243034e-11, 4.759114684966798e-11, 4.434543063452062e-11, 4.131663772218286e-11, 3.849057970198555e-11, 3.585397499239954e-11, 3.339439230792093e-11, 3.1100197556366764e-11, 2.8960503964488732e-11, 2.6965125241335458e-11, 2.5104531599683314e-11, 2.3369808466145196e-11, 2.1752617720287947e-11, 2.0245161312271993e-11, 1.8840147117200317e-11, 1.7530756892554894e-11, 1.6310616212833074e-11, 1.517376626279861e-11, 1.4114637377655267e-11, 1.3128024224957164e-11, 1.2209062529210162e-11, 1.1353207245912529e-11, 1.0556212097249469e-11, 9.81411038681283e-12, 9.123197015581207e-12, 8.480011625982733e-12, 7.881322805188352e-12, 7.324113282861535e-12, 6.805566062434938e-12, 6.323051428608301e-12, 5.874114777177112e-12, 5.45646521651982e-12, 5.067964893103608e-12, 4.706618996225321e-12, 4.370566399894833e-12, 4.058070902302353e-12, 3.767513025697504e-12, 3.497382341754993e-12, 3.246270289616985e-12, 3.012863455793524e-12, 2.7959372869763898e-12, 2.594350208585392e-12, 2.4070381235255263e-12, 2.233009267194631e-12, 2.0713393962497396e-12, 1.9211672900216335e-12, 1.781690544766063e-12, 1.6521616421615701e-12, 1.5318842746122078e-12, 1.420209910992978e-12, 1.3165345874904905e-12, 1.2202959091449357e-12, 1.1309702485955264e-12, 1.0480701293734742e-12, 9.711417818774612e-13, 8.997628609094678e-13, 8.33540314346531e-13, 7.721083931791917e-13, 7.151267937625673e-13, 6.622789237035136e-13, 6.132702833494536e-13, 5.67826955353259e-13, 5.256941952660472e-13, 4.866351165577799e-13, 4.5042946388588106e-13, 4.168724688263829e-13, 3.8577378265193477e-13, 3.5695648108783525e-13, 3.3025613630246516e-13, 3.0551995169341054e-13, 2.826059553164048e-13, 2.613822480721428e-13, 2.4172630301711766e-13, 2.235243123999287e-13, 2.0667057924497407e-13, 1.9106695051197852e-13, 1.7662228905327294e-13, 1.6325198177194018e-13, 1.5087748155362713e-13, 1.3942588070370684e-13, 1.2882951377022513e-13, 1.19025587772311e-13, 1.0995583798406089e-13, 1.0156620754587704e-13, 9.380654928937203e-14, 8.663034826873507e-14, 7.999446359134996e-14, 7.385888823389342e-14, 6.818652561753041e-14, 6.294298179754232e-14, 5.809637219913156e-14, 5.3617141902578524e-14, 4.9477898547699457e-14, 4.565325698996229e-14, 4.2119694898939206e-14, 3.8855418544271416e-14, 3.5840238065235954e-14, 3.305545156756712e-14, 3.0483737435610395e-14, 2.8109054289376347e-14, 2.5916548054804993e-14, 2.389246565172472e-14, 2.2024074837759845e-14, 2.029958977796325e-14, 1.8708101939370932e-14, 1.7239515937130313e-14, 1.588448998447187e-14, 1.4634380622693935e-14, 1.3481191429625654e-14, 1.2417525425828191e-14, 1.1436540917187764e-14, 1.0531910530638212e-14, 9.697783216611665e-15, 8.928749007534359e-15, 8.219806336336131e-15, 7.566331732597409e-15, 6.964051726682655e-15, 6.409016804066048e-15, 5.8975772631015774e-15, 5.426360839779762e-15, 4.992251972597381e-15, 4.5923725895923305e-15, 4.224064307907025e-15, 3.884871943982091e-15, 3.572528239685931e-15, 3.284939716391213e-15, 3.0201735752500812e-15, 2.7764455677273998e-15, 2.552108765855026e-15, 2.3456431666973544e-15, 2.155646070194869e-15, 1.9808231739020653e-15, 1.8199803321812173e-15, 1.6720159311748843e-15, 1.5359138343772486e-15, 1.4107368568755029e-15, 1.295620729354462e-15, 1.1897685157662023e-15, 1.092445451176527e-15, 1.0029741687251695e-15, 9.207302868897057e-16, 8.451383303360608e-16, 7.756679595824366e-16, 7.118304865088021e-16, 6.531756544204767e-16, 5.992886629307874e-16, 5.497874193727044e-16, 5.043199997905818e-16, 4.625623038079216e-16, 4.242158888222449e-16, 3.8900597004999857e-16, 3.566795739388437e-16, 3.270038333871319e-16, 2.997644140659838e-16, 2.7476406193288076e-16, 2.518212627614852e-16, 2.3076900519462503e-16, 2.1145363945983492e-16, 1.9373382447310074e-16, 1.774795565998076e-16, 1.6257127384542378e-16, 1.4889902971502364e-16, 1.3636173141301864e-16, 1.2486643745490116e-16, 1.143277101337075e-16, 1.0466701862740922e-16, 9.58121888515304e-17, 8.769689645580578e-17, 8.026019963635345e-17, 7.344610868722705e-17, 6.720318944881275e-17, 6.148419802671879e-17, 5.624574435484235e-17, 5.1447982361374295e-17, 4.705432466771293e-17, 4.303117990862347e-17, 3.9347710908445304e-17, 3.597561208359154e-17, 3.2888904566819384e-17, 3.00637476645296e-17, 2.7478265365380055e-17, 2.511238671742083e-17, 2.294769898237915e-17, 2.096731256019983e-17, 1.9155736754998562e-17, 1.7498765525691186e-17, 1.598337243116633e-17, 1.459761404138421e-17, 1.333054114259206e-17, 1.217211711730081e-17, 1.111314292809564e-17, 1.014518817905851e-17, 9.260527769844609e-18, 8.452083695537174e-18, 7.713371580547225e-18, 7.038451567249343e-18, 6.4218832099583e-18, 5.858684052444957e-18, 5.34429159264029e-18, 4.874528361647208e-18, 4.445569865822254e-18, 4.0539151606433135e-18, 3.696359843474573e-18, 3.369971269295768e-18, 3.072065809090022e-18, 2.800187984985555e-18, 2.5520913295158347e-18, 2.3257208285877803e-18, 2.1191968190091856e-18, 1.9308002217992692e-18, 1.7589590020591342e-18, 1.6022357549756653e-18, 1.459316325631836e-18, 1.328999377752913e-18, 1.2101868333818025e-18, 1.1018751117941311e-18, 1.0031471017773769e-18, 9.131648067479462e-19, 8.311626071019807e-19, 7.564410887235126e-19, 6.883613907383966e-19, 6.26340029432867e-19, 5.698441587780224e-19, 5.183872312403669e-19, 4.715250255362377e-19, 4.288520107252082e-19, 3.8999801855350434e-19, 3.546251982707015e-19, 3.224252302677218e-19, 2.9311677683638587e-19, 2.6644315014427476e-19, 2.4217017916613137e-19, 2.200842588261802e-19, 1.9999056599534992e-19, 1.8171142826337494e-19, 1.6508483257727612e-19, 1.4996306191319762e-19, 1.362114491357574e-19, 1.2370723810509487e-19, 1.1233854292323664e-19, 1.0200339697429888e-19, 9.260888411296596e-20, 8.40703449977505e-20, 7.631065215445812e-20, 6.925954789538619e-20, 6.285303971506296e-20, 5.703284823744813e-20, 5.1745903205836687e-20, 4.694388338833504e-20, 4.2582796621558714e-20, 3.8622596535753756e-20, 3.502683279827508e-20, 3.1762331981475844e-20, 2.879890640760415e-20, 2.610908854913392e-20, 2.3667888769789898e-20, 2.1452574380937562e-20, 1.9442468161443712e-20, 1.7618764647900293e-20, 1.5964362647462458e-20, 1.4463712558600987e-20, 1.3102677206836635e-20, 1.1868405013952623e-20, 1.0749214421136891e-20, 9.734488579781568e-21, 8.81457940899221e-21, 7.980720196901547e-21, 7.22494599425497e-21, 6.540021113999599e-21, 5.919373110281037e-21, 5.3570326648043386e-21, 4.847578858381446e-21, 4.386089351063533e-21, 3.968095035907178e-21, 3.589538769481633e-21, 3.246737816996137e-21, 2.936349681691306e-21, 2.655341017153448e-21, 2.4009593477101476e-21, 2.1707073462647447e-21, 1.9623194410233294e-21, 1.7737405427409873e-21, 1.6031067025293043e-21, 1.4487275270757322e-21, 1.3090701934656315e-21, 1.1827449197961981e-21, 1.0684917605438854e-21, 9.651686072992917e-22, 8.717402861127202e-22, 7.872686523883977e-22, 7.109035931067232e-22, 6.418748542160604e-22, 5.794846183862594e-22, 5.231007650172763e-22, 4.721507505045936e-22, 4.261160523303956e-22, 3.84527125622949e-22, 3.4695882544958687e-22, 3.1302625232061725e-22, 2.8238098221842883e-22, 2.547076459610228e-22, 2.2972082589224646e-22, 2.0716224078962784e-22, 1.867981925200074e-22, 1.6841725037603867e-22, 1.5182815121399886e-22, 1.3685789550429856e-22, 1.233500212180243e-22, 1.1116303912168348e-22, 1.0016901455254899e-22, 9.025228211184665e-23, 8.130828095456265e-23, 7.324249948387004e-23, 6.596951928512176e-23, 5.941214906818065e-23, 5.350064023589667e-23, 4.817197646840136e-23, 4.336923041450175e-23, 3.904098121913447e-23, 3.514078719534294e-23, 3.1626708475808646e-23]\n"
     ]
    }
   ],
   "source": [
    "n_directions = 6\n",
    "direction_embedding_dimension = 32\n",
    "time_embedding_dimension = 32\n",
    "T = 1000\n",
    "beta_1 = 10**-4\n",
    "beta_T = 10**-2\n",
    "beta_1_tensor = tch.tensor(beta_1)\n",
    "height = 28\n",
    "width = 28\n",
    "# list containing \\bar{alpha_t}\n",
    "diffusion_scheduler = [1]\n",
    "for i in range(1,T+1):\n",
    "    diffusion_scheduler.append(diffusion_scheduler[i-1]*(1-i*beta_1))\n",
    "print(diffusion_scheduler)\n",
    "diffusion_scheduler = tch.tensor(diffusion_scheduler).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeEmbedding(nn.Module):\n",
    "#     def __init__(self, embed_dim: int):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "\n",
    "#     def forward(self, t):\n",
    "#         # t: (batch_size,) - the timestep\n",
    "#         # Create the sinusoidal embedding\n",
    "#         half_dim = self.embed_dim // 2\n",
    "#         exponents = torch.arange(half_dim, dtype=torch.float32) / half_dim\n",
    "#         freqs = torch.pow(10000, -exponents).to(t.device)\n",
    "#         angles = t[:, None] * freqs  # Broadcasting over the batch dimension\n",
    "#         # Combine sine and cosine\n",
    "#         time_embedding = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "#         return time_embedding  # Shape: (batch_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define architecture and U-net\n",
    "# Unet is predicting the noise.\n",
    "#  I have image x. What image might I get if I denoise by time \"timestep\" in the past in given direction.\n",
    "\n",
    "\n",
    "# for now the same as direction embedding\n",
    "class TimeEmbedding(tch.nn.Module):\n",
    "    def __init__(self, time_embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.time_embedding = tch.nn.Linear(1, time_embedding_dimension)\n",
    "    def forward(self, timestep):\n",
    "        timestep = timestep.view(-1,1).float()\n",
    "        return self.time_embedding(timestep)\n",
    "\n",
    "class DirectionEmbedding(tch.nn.Module):\n",
    "    def __init__(self, n_classes, direction_embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.direction_embedding = tch.nn.Embedding(n_classes, direction_embedding_dimension)\n",
    "    def forward(self, class_label):\n",
    "         return self.direction_embedding(class_label)\n",
    "\n",
    "# a block in my modified UNet\n",
    "class Block(tch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_embedding_dimension, direction_embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.conv = tch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm = tch.nn.BatchNorm2d(out_channels)\n",
    "        self.activation = tch.nn.ReLU()\n",
    "        # rescale time_embedding and direction_embedding to match the dimension of the channels\n",
    "        self.time_embedding_projection = tch.nn.Linear(time_embedding_dimension, in_channels)\n",
    "        self.direction_embedding_projection = tch.nn.Linear(direction_embedding_dimension, in_channels)\n",
    "    def forward(self, x, time_embedding, direction_embedding):\n",
    "        \n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # why -1 instead of n_channels?\n",
    "        #  should be broadcastable to x.\n",
    "        time_embedding = self.time_embedding_projection(time_embedding).view(batch_size,-1,1,1)\n",
    "        direction_embedding = self.direction_embedding_projection(direction_embedding).view(batch_size,-1,1,1)\n",
    "\n",
    "        # adding time embedding to input\n",
    "        x = x+time_embedding+direction_embedding\n",
    "        \n",
    "        # forward pass\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 3\n",
    "feature2 = 4\n",
    "class modifiedUnet(tch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_embedding_dimension, direction_embedding_dimension, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_embedding = TimeEmbedding(time_embedding_dimension)\n",
    "        self.direction_embedding = DirectionEmbedding(n_classes, direction_embedding_dimension)\n",
    "        self.pool = tch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        self.block1 = Block(in_channels, feature1, time_embedding_dimension,direction_embedding_dimension)\n",
    "        # maxpool\n",
    "        self.block2 = Block(feature1, feature2, time_embedding_dimension,direction_embedding_dimension)\n",
    "        # maxpool\n",
    "        self.block3 = Block(feature2, feature2, time_embedding_dimension,direction_embedding_dimension)\n",
    "        \n",
    "        # upsample\n",
    "        self.up1 = tch.nn.ConvTranspose2d(feature2,feature2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.block4 = Block(feature2, feature1, time_embedding_dimension,direction_embedding_dimension)\n",
    "        \n",
    "        # upsample\n",
    "        self.up2 = tch.nn.ConvTranspose2d(feature1,feature1, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.block5 = Block(feature1, out_channels, time_embedding_dimension,direction_embedding_dimension)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, class_label, timestep):\n",
    "        time_embedding = self.time_embedding(timestep)\n",
    "        direction_embedding = self.direction_embedding(class_label)\n",
    "\n",
    "        # no skip connections for now\n",
    "        x1 = self.block1(x,time_embedding,direction_embedding)\n",
    "        # downsample\n",
    "        x1 = self.pool(x1)\n",
    "        x2 =  self.block2(x1,time_embedding,direction_embedding)\n",
    "        # downsample\n",
    "        x2 = self.pool(x2)\n",
    "        x3 =  self.block3(x2,time_embedding,direction_embedding)\n",
    "        # upsample\n",
    "        x3 = self.up1(x3)\n",
    "        # skip connection below\n",
    "        #  x3 = x3+x2\n",
    "        x4 = self.block4(x3,time_embedding,direction_embedding)\n",
    "\n",
    "\n",
    "        # upsample\n",
    "        x4 = self.up2(x4)\n",
    "        # skip connection below\n",
    "        # x4 = x4+x1\n",
    "        x5 = self.block5(x4, time_embedding, direction_embedding)\n",
    "        return x5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.0786e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.1865e+00, 1.5924e+00, 0.0000e+00, 6.1459e-01, 3.1850e-01,\n",
      "           6.8335e-01, 3.2028e-01, 6.8130e-01, 2.3418e-01, 6.4626e-01,\n",
      "           1.7764e-01, 6.2136e-01, 2.1784e-01, 6.4037e-01, 2.5927e-01,\n",
      "           6.3061e-01, 2.7346e-01, 6.8773e-01, 2.2482e-01, 6.3580e-01,\n",
      "           2.6315e-01, 6.4600e-01, 2.9823e-01, 7.2086e-01, 1.8275e-01,\n",
      "           5.4030e-01, 1.7305e-01, 0.0000e+00],\n",
      "          [2.6858e+00, 0.0000e+00, 8.8815e-01, 7.5723e-03, 0.0000e+00,\n",
      "           1.7849e-02, 8.6411e-04, 0.0000e+00, 4.4642e-02, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 6.3368e-03, 0.0000e+00, 4.0735e-03, 0.0000e+00,\n",
      "           1.9726e-02, 1.7878e-02, 0.0000e+00, 0.0000e+00, 5.1569e-02,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.0199e+00, 1.2257e+00, 0.0000e+00, 8.1986e-02, 9.1499e-02,\n",
      "           1.5846e-01, 0.0000e+00, 1.1509e-01, 9.6027e-04, 7.6267e-02,\n",
      "           1.9060e-03, 1.2613e-02, 1.3097e-02, 1.4199e-01, 1.4725e-02,\n",
      "           6.5312e-02, 3.6292e-03, 1.4944e-01, 2.2661e-02, 1.2681e-01,\n",
      "           6.6452e-02, 1.0797e-01, 0.0000e+00, 7.8113e-02, 0.0000e+00,\n",
      "           1.8446e-02, 1.9317e-02, 0.0000e+00],\n",
      "          [2.6810e+00, 0.0000e+00, 8.4722e-01, 2.6569e-02, 7.2434e-02,\n",
      "           1.4378e-02, 3.0729e-02, 3.7163e-03, 0.0000e+00, 1.0326e-02,\n",
      "           6.1205e-02, 0.0000e+00, 0.0000e+00, 9.0553e-03, 2.1165e-02,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 6.1876e-02, 0.0000e+00, 0.0000e+00,\n",
      "           1.0523e-02, 2.9404e-02, 0.0000e+00],\n",
      "          [2.0758e+00, 1.2150e+00, 0.0000e+00, 8.6164e-02, 1.3684e-01,\n",
      "           2.4269e-01, 7.9197e-02, 1.0009e-01, 3.3563e-02, 8.8139e-02,\n",
      "           3.8325e-02, 0.0000e+00, 1.0064e-01, 1.9315e-01, 2.7275e-02,\n",
      "           2.0115e-02, 3.3309e-02, 1.2693e-01, 0.0000e+00, 4.5303e-02,\n",
      "           0.0000e+00, 1.6417e-01, 7.3794e-02, 7.4600e-02, 0.0000e+00,\n",
      "           1.0272e-02, 2.4175e-03, 0.0000e+00],\n",
      "          [2.6404e+00, 0.0000e+00, 9.9186e-01, 7.2501e-02, 0.0000e+00,\n",
      "           0.0000e+00, 3.6191e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           6.2183e-02, 2.7506e-02, 0.0000e+00, 0.0000e+00, 1.0667e-01,\n",
      "           7.6570e-03, 0.0000e+00, 0.0000e+00, 1.3283e-01, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 3.7459e-02, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 6.0134e-03, 0.0000e+00],\n",
      "          [2.0244e+00, 1.2727e+00, 0.0000e+00, 2.9630e-01, 1.7278e-01,\n",
      "           2.4714e-01, 0.0000e+00, 1.4151e-01, 2.8665e-02, 1.1372e-01,\n",
      "           1.1587e-01, 1.3207e-01, 3.8658e-02, 1.5937e-01, 2.1078e-02,\n",
      "           6.4021e-02, 0.0000e+00, 8.0335e-02, 1.1122e-01, 1.5107e-01,\n",
      "           3.2525e-03, 5.3981e-02, 5.4545e-02, 3.0310e-01, 0.0000e+00,\n",
      "           0.0000e+00, 6.3370e-03, 0.0000e+00],\n",
      "          [2.7020e+00, 0.0000e+00, 6.6013e-01, 4.5293e-02, 2.5132e-01,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 8.3711e-03, 8.7010e-02, 0.0000e+00, 0.0000e+00,\n",
      "           1.5731e-02, 7.2754e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           1.7459e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1488e-02,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.1442e+00, 1.2601e+00, 0.0000e+00, 1.8617e-01, 6.7026e-02,\n",
      "           2.3827e-01, 0.0000e+00, 6.2750e-02, 0.0000e+00, 6.8857e-02,\n",
      "           2.4875e-03, 1.7709e-01, 2.2399e-02, 8.1363e-02, 1.2726e-02,\n",
      "           1.2188e-01, 2.8537e-02, 0.0000e+00, 0.0000e+00, 1.5977e-01,\n",
      "           3.1537e-02, 6.2672e-02, 1.0581e-01, 2.6872e-01, 1.9151e-03,\n",
      "           0.0000e+00, 8.9177e-03, 0.0000e+00],\n",
      "          [2.5437e+00, 0.0000e+00, 9.6179e-01, 3.9654e-02, 0.0000e+00,\n",
      "           0.0000e+00, 9.4848e-02, 0.0000e+00, 0.0000e+00, 3.4047e-03,\n",
      "           8.6366e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 2.1825e-02, 7.3973e-02, 0.0000e+00,\n",
      "           0.0000e+00, 1.0549e-02, 4.6304e-03, 0.0000e+00, 0.0000e+00,\n",
      "           8.4816e-03, 5.5942e-03, 0.0000e+00],\n",
      "          [1.9502e+00, 1.2964e+00, 0.0000e+00, 1.2056e-01, 6.2310e-02,\n",
      "           1.4134e-01, 0.0000e+00, 7.6510e-02, 6.5620e-03, 2.4814e-02,\n",
      "           1.8836e-01, 3.4226e-01, 1.6480e-02, 3.4580e-02, 0.0000e+00,\n",
      "           1.0243e-01, 6.1977e-03, 9.9769e-02, 1.6438e-01, 1.7097e-01,\n",
      "           0.0000e+00, 7.2917e-02, 0.0000e+00, 2.1131e-01, 0.0000e+00,\n",
      "           0.0000e+00, 1.4073e-02, 0.0000e+00],\n",
      "          [2.7187e+00, 0.0000e+00, 7.1193e-01, 3.5434e-02, 2.0699e-01,\n",
      "           0.0000e+00, 0.0000e+00, 1.5752e-02, 7.4604e-02, 5.4784e-03,\n",
      "           0.0000e+00, 2.3983e-02, 2.3466e-01, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 2.2463e-02, 0.0000e+00, 0.0000e+00,\n",
      "           1.2863e-01, 0.0000e+00, 1.0181e-03, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [2.1298e+00, 1.1988e+00, 0.0000e+00, 1.8048e-01, 1.6701e-01,\n",
      "           1.6204e-01, 9.8457e-02, 1.2337e-01, 5.6198e-02, 2.3772e-02,\n",
      "           6.8035e-02, 3.3851e-01, 6.6181e-02, 7.1463e-02, 0.0000e+00,\n",
      "           7.3622e-02, 5.6621e-05, 1.0854e-01, 2.8979e-02, 1.6438e-01,\n",
      "           5.9720e-02, 1.1201e-01, 1.2702e-01, 2.5862e-01, 8.1598e-03,\n",
      "           1.0890e-02, 4.1647e-02, 0.0000e+00],\n",
      "          [2.5761e+00, 0.0000e+00, 9.6301e-01, 8.4177e-02, 0.0000e+00,\n",
      "           1.8040e-02, 7.1690e-02, 0.0000e+00, 0.0000e+00, 6.2494e-02,\n",
      "           5.1937e-04, 5.7776e-02, 8.7230e-02, 1.5767e-02, 2.9599e-02,\n",
      "           0.0000e+00, 0.0000e+00, 9.6467e-03, 1.0198e-01, 4.7922e-03,\n",
      "           0.0000e+00, 1.4776e-02, 0.0000e+00, 1.1678e-02, 2.6053e-02,\n",
      "           1.2896e-02, 5.7741e-03, 0.0000e+00],\n",
      "          [2.0118e+00, 1.3134e+00, 0.0000e+00, 3.1367e-01, 2.0233e-01,\n",
      "           2.0860e-01, 3.1169e-02, 1.7591e-01, 4.6038e-02, 1.5684e-01,\n",
      "           1.0349e-01, 1.6488e-01, 0.0000e+00, 4.8121e-02, 3.6175e-02,\n",
      "           1.0952e-01, 2.8689e-02, 1.3291e-01, 1.7184e-01, 2.1177e-01,\n",
      "           9.8601e-04, 8.7761e-02, 0.0000e+00, 1.2400e-01, 0.0000e+00,\n",
      "           0.0000e+00, 2.3286e-02, 0.0000e+00],\n",
      "          [2.7075e+00, 0.0000e+00, 6.4157e-01, 2.6784e-02, 2.2104e-01,\n",
      "           1.7764e-02, 3.9837e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           2.2173e-03, 3.6880e-02, 3.8450e-02, 0.0000e+00, 0.0000e+00,\n",
      "           1.1932e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 1.8539e-02, 0.0000e+00],\n",
      "          [2.1504e+00, 1.2729e+00, 0.0000e+00, 2.2543e-01, 5.1287e-02,\n",
      "           2.2371e-01, 7.7451e-02, 1.5623e-01, 0.0000e+00, 1.8200e-01,\n",
      "           0.0000e+00, 1.4155e-01, 3.4290e-03, 1.5438e-01, 1.4571e-02,\n",
      "           3.0287e-02, 0.0000e+00, 7.6785e-02, 0.0000e+00, 1.4456e-01,\n",
      "           0.0000e+00, 1.3947e-01, 8.5071e-02, 1.8933e-01, 0.0000e+00,\n",
      "           0.0000e+00, 1.6465e-02, 0.0000e+00],\n",
      "          [2.5726e+00, 0.0000e+00, 9.5322e-01, 3.4241e-02, 0.0000e+00,\n",
      "           5.7456e-02, 1.7510e-01, 6.3049e-03, 0.0000e+00, 0.0000e+00,\n",
      "           9.3733e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.3013e-02,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1077e-01, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "          [1.9949e+00, 1.3569e+00, 0.0000e+00, 5.4270e-02, 4.4200e-02,\n",
      "           1.5480e-01, 1.4227e-01, 2.4965e-01, 7.5401e-04, 2.8751e-02,\n",
      "           0.0000e+00, 1.6106e-02, 0.0000e+00, 5.3621e-02, 5.7315e-04,\n",
      "           9.4616e-02, 0.0000e+00, 9.5764e-02, 6.8539e-02, 6.1378e-02,\n",
      "           0.0000e+00, 6.5023e-02, 0.0000e+00, 1.8576e-01, 0.0000e+00,\n",
      "           0.0000e+00, 3.8736e-02, 0.0000e+00],\n",
      "          [2.7048e+00, 0.0000e+00, 8.2743e-01, 0.0000e+00, 1.0769e-01,\n",
      "           6.3708e-02, 0.0000e+00, 7.7852e-02, 2.7209e-01, 2.4717e-02,\n",
      "           0.0000e+00, 4.7599e-03, 4.0873e-03, 0.0000e+00, 4.3695e-02,\n",
      "           5.4050e-02, 3.8504e-02, 1.8127e-02, 0.0000e+00, 5.2047e-03,\n",
      "           1.5053e-01, 2.5223e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           1.4761e-02, 1.2908e-02, 0.0000e+00],\n",
      "          [2.0508e+00, 1.2584e+00, 0.0000e+00, 1.4116e-01, 1.6357e-01,\n",
      "           2.7455e-01, 2.0630e-02, 9.5165e-02, 4.8131e-02, 6.6394e-02,\n",
      "           1.5795e-01, 1.5171e-01, 4.8354e-02, 0.0000e+00, 1.3800e-01,\n",
      "           1.7784e-01, 7.7489e-02, 0.0000e+00, 7.8492e-02, 1.5647e-01,\n",
      "           1.8557e-01, 1.3766e-01, 1.0461e-01, 1.3010e-01, 4.5816e-02,\n",
      "           8.3304e-02, 4.8845e-02, 0.0000e+00],\n",
      "          [2.6849e+00, 0.0000e+00, 1.0142e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 9.2649e-02, 4.6735e-02, 0.0000e+00, 8.4578e-02,\n",
      "           6.1261e-02, 2.5913e-02, 1.4601e-01, 5.6882e-02, 0.0000e+00,\n",
      "           1.2518e-02, 7.1140e-02, 9.5073e-02, 0.0000e+00, 7.8902e-02,\n",
      "           0.0000e+00, 4.8235e-02, 2.6771e-02, 5.1990e-04, 0.0000e+00,\n",
      "           2.7641e-02, 6.3575e-02, 0.0000e+00],\n",
      "          [1.9732e+00, 1.1416e+00, 0.0000e+00, 6.9287e-02, 0.0000e+00,\n",
      "           3.0768e-01, 0.0000e+00, 9.5840e-02, 7.2058e-02, 1.9035e-01,\n",
      "           1.5951e-01, 2.2731e-01, 9.5577e-02, 1.9366e-02, 0.0000e+00,\n",
      "           2.5116e-01, 0.0000e+00, 8.0176e-02, 6.1308e-02, 1.9275e-02,\n",
      "           0.0000e+00, 1.4014e-01, 0.0000e+00, 6.8655e-02, 0.0000e+00,\n",
      "           7.7447e-02, 3.0405e-02, 0.0000e+00],\n",
      "          [2.6707e+00, 0.0000e+00, 8.7365e-01, 4.3702e-02, 1.9823e-01,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0470e-02,\n",
      "           1.8244e-01, 9.7171e-03, 9.8384e-02, 6.4263e-04, 1.0023e-01,\n",
      "           0.0000e+00, 0.0000e+00, 3.8947e-03, 1.4852e-01, 0.0000e+00,\n",
      "           0.0000e+00, 1.0084e-02, 1.2824e-01, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 1.1948e-01, 0.0000e+00],\n",
      "          [2.0539e+00, 1.3094e+00, 0.0000e+00, 6.1753e-01, 0.0000e+00,\n",
      "           6.5271e-01, 0.0000e+00, 7.9414e-01, 0.0000e+00, 6.6891e-01,\n",
      "           0.0000e+00, 7.2492e-01, 0.0000e+00, 7.2512e-01, 0.0000e+00,\n",
      "           7.5367e-01, 0.0000e+00, 7.8243e-01, 0.0000e+00, 7.6437e-01,\n",
      "           0.0000e+00, 6.8699e-01, 0.0000e+00, 7.0838e-01, 0.0000e+00,\n",
      "           6.8709e-01, 0.0000e+00, 0.0000e+00],\n",
      "          [2.6067e+00, 1.1778e+00, 2.9366e+00, 1.1453e+00, 2.5110e+00,\n",
      "           1.1540e+00, 2.5208e+00, 1.1734e+00, 2.4818e+00, 1.1345e+00,\n",
      "           2.4965e+00, 1.1448e+00, 2.5503e+00, 1.1432e+00, 2.5374e+00,\n",
      "           1.1417e+00, 2.5414e+00, 1.1287e+00, 2.5608e+00, 1.1575e+00,\n",
      "           2.5472e+00, 1.1236e+00, 2.5407e+00, 1.1416e+00, 2.5751e+00,\n",
      "           1.4045e+00, 2.2117e+00, 0.0000e+00]]]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = modifiedUnet(in_channels=1, out_channels=1, time_embedding_dimension=16, direction_embedding_dimension=16, n_classes=10)\n",
    "\n",
    "# # Dummy inputs\n",
    "# x = tch.randn(1, 1, 28, 28)  # Batch size = 1, Channels = 1, Height = 28, Width = 28\n",
    "# class_label = tch.tensor([3])  # Class index for the direction embedding\n",
    "# timestep = tch.tensor([5.0])  # Scalar timestep\n",
    "\n",
    "# # Forward pass\n",
    "# output = model(x, class_label, timestep)\n",
    "# print(output.shape)  # Should match the shape defined by the last Block (e.g., [1, 1, 28, 28] if out_channels=1)``\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiedUnet = modifiedUnet(\n",
    "    in_channels=1, \n",
    "    out_channels=1,\n",
    "    time_embedding_dimension=16,\n",
    "    direction_embedding_dimension=16, \n",
    "    n_classes=10).to(device)\n",
    "#weights, loss function, optimizer\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "# optimizer and scheduler\n",
    "optimizer = tch.optim.AdamW(modifiedUnet.parameters(), lr=1e-4,\n",
    "                            weight_decay=0.1)\n",
    "scheduler = tch.optim.lr_scheduler.StepLR(optimizer, step_size=10000,\n",
    "                                          gamma=0.2)\n",
    "loss_func = tch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tch.randint(low=0,high=T, size=(batch_size,))\n",
    "a.shape\n",
    "noise = tch.randn(batch_size, 1, height, width, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "running_loss = 0.0\n",
    "epoch_loss_ = 0.0\n",
    "epoch_loss = 0.0\n",
    "n_epoch = 3\n",
    "\n",
    " \n",
    "for epoch in range(n_epoch):\n",
    "    i = 0\n",
    "    for data in train_loader:\n",
    "        ###### COMPLETER ICI ######\n",
    "        loss_val = 0 # requis aux lignes suivantes\n",
    "        images,labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        timesteps = tch.randint(1, T, size=(batch_size,), device=device)  # Move timesteps to the device\n",
    "        # generating a batch of random noise\n",
    "\n",
    "        noise = tch.randn(batch_size, 1, height, width, device=device)\n",
    "        alpha_bar = diffusion_scheduler[timesteps].view(-1,1,1,1).to(device)\n",
    "        noised_image = tch.sqrt(alpha_bar)*images+tch.sqrt(1-alpha_bar)*noise\n",
    "        predicted_noise = modifiedUnet(noised_image,labels, timesteps)\n",
    "        loss_val = loss_func(noise, predicted_noise)\n",
    "        ## Gradient calculation\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        #####\n",
    "\n",
    "        running_loss += loss_val.item()\n",
    "        epoch_loss += loss_val.item()\n",
    "        print(f\"Epoch = {epoch}\", end=\"\\r\", flush=True)\n",
    "    if epoch % 10 == 0:    # every 100 epoch...\n",
    "        # disp_loss(epoch_loss, epoch)\n",
    "        pass\n",
    "    i = i+1\n",
    "    epoch_loss = 0.0\n",
    "    scheduler.step()  \n",
    "\n",
    "#save the model weights after training\n",
    "tch.save(modifiedUnet.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "tensor([[[[ 0.2843, -0.1254,  0.3258,  ...,  0.4570, -0.1362, -0.1375],\n",
      "          [ 0.2731, -0.0479, -0.3370,  ...,  0.2387,  0.4787,  0.2098],\n",
      "          [ 0.3362, -0.2064, -0.2961,  ..., -0.0840, -0.0472, -0.2859],\n",
      "          ...,\n",
      "          [-0.3799,  0.2064, -0.0335,  ..., -0.2411, -0.2498, -0.1472],\n",
      "          [ 0.3017,  0.2287,  0.1740,  ..., -0.1616, -0.1287,  0.6310],\n",
      "          [-0.3150,  0.3799,  0.3195,  ...,  0.0925, -0.0479,  0.1493]]],\n",
      "\n",
      "\n",
      "        [[[-0.3271,  0.2176,  0.0902,  ..., -0.2250,  0.2685, -0.4917],\n",
      "          [-0.6474,  0.1665, -0.4795,  ..., -0.0912,  0.4784,  0.4119],\n",
      "          [ 0.2659,  0.2122,  0.3020,  ..., -0.3189, -0.2113, -0.3505],\n",
      "          ...,\n",
      "          [-0.0154, -0.2211,  0.2403,  ...,  0.1412, -0.2264, -0.1517],\n",
      "          [ 0.3163,  0.2225,  0.0846,  ...,  0.0016,  0.1408,  0.0258],\n",
      "          [-0.0987,  0.2383, -0.0798,  ..., -0.1580, -0.0022, -0.2956]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4175,  0.3033,  0.2268,  ..., -0.3193, -0.2854, -0.0948],\n",
      "          [-0.0125, -0.1745, -0.0377,  ...,  0.5500, -1.0000, -0.0831],\n",
      "          [-0.2054,  0.1070, -0.0287,  ...,  0.0272,  0.2957, -0.1305],\n",
      "          ...,\n",
      "          [ 0.3135, -0.1384,  0.5093,  ...,  0.0372,  0.1523, -0.0078],\n",
      "          [-0.5938, -0.1026,  0.0011,  ..., -0.1505, -0.3255, -0.2930],\n",
      "          [ 0.2693,  0.1990,  0.2167,  ..., -0.4800, -0.0322, -0.1909]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4980,  0.1314,  0.0593,  ...,  0.0445,  0.0841,  0.2324],\n",
      "          [ 0.5682, -0.0947, -0.0842,  ..., -0.1575, -0.1231,  0.3330],\n",
      "          [ 0.0041,  0.1294, -0.0311,  ..., -0.4137, -0.1202,  0.0330],\n",
      "          ...,\n",
      "          [-0.0442,  0.5055, -0.2993,  ...,  0.4209,  0.1708, -0.1416],\n",
      "          [ 0.5491, -0.3618, -0.5043,  ...,  0.1989,  0.3181, -0.0668],\n",
      "          [-0.2595,  0.0137,  0.1181,  ..., -0.1729,  0.4916,  0.0840]]],\n",
      "\n",
      "\n",
      "        [[[-0.3245,  0.1084, -0.1834,  ..., -0.2537, -0.2725,  0.1985],\n",
      "          [ 0.5324, -0.1453, -0.3734,  ...,  0.1861,  0.2274, -0.0938],\n",
      "          [ 0.4564,  0.3031,  0.5450,  ...,  0.1257,  0.2983,  0.1738],\n",
      "          ...,\n",
      "          [ 0.3487, -0.0595,  0.2458,  ..., -0.2219,  0.1565, -0.4116],\n",
      "          [ 0.0151,  0.4462, -0.1680,  ..., -0.0171, -0.1976, -0.0127],\n",
      "          [-0.3137,  0.3914, -0.1562,  ...,  0.3494,  0.1477, -0.0037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0478,  0.5394, -0.0903,  ..., -0.5625,  0.0417,  0.5676],\n",
      "          [-0.1994, -0.1703, -0.0367,  ...,  0.2709, -0.0570,  0.0492],\n",
      "          [-0.2577,  0.2229,  0.5839,  ..., -0.1192,  0.3426, -0.0796],\n",
      "          ...,\n",
      "          [-0.3311,  0.2026, -0.2114,  ..., -0.0135,  0.0775,  0.0309],\n",
      "          [-0.1317, -0.2629,  0.1256,  ...,  0.1734,  0.3617,  0.3148],\n",
      "          [-0.2698,  0.0571,  0.4171,  ..., -0.1431,  0.1920,  0.1456]]]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36736/3455503051.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = tch.tensor(labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fba14e4bac0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_data(images):\n",
    "    # for each image take a maximu of absolute values. Look at channels, height and width\n",
    "    # thus each image gets scaled individually\n",
    "    max_vals = tch.amax(tch.abs(images), dim=(1, 2, 3), keepdim=True)\n",
    "    # images are between [0,1]\n",
    "    images = images/max_vals\n",
    "    return images\n",
    "# how to generate image from noise\n",
    "# algorith from DDPM paper\n",
    "def generate_image(n_images,labels, noise_predictor):\n",
    "    noise_predictor = noise_predictor.cpu()\n",
    "    labels = tch.tensor(labels)\n",
    "    images = tch.randn(n_images,1,height,width)\n",
    "    # normalize images\n",
    "    images = normalize_data(images)\n",
    "    i = T\n",
    "    while(i>0):\n",
    "        times = tch.tensor([i] * n_images)\n",
    "        alpha_bar = diffusion_scheduler[i]\n",
    "        # sigma_t * z term \n",
    "        if i>1:\n",
    "            noise = tch.sqrt(i*beta_1_tensor)*tch.randn(n_images,1, height, width)\n",
    "        else:\n",
    "            noise = images*0\n",
    "        # i*beta = 1-alphta_t = beta_t\n",
    "        images = (1/tch.sqrt(alpha_bar))*(images-i*beta_1_tensor/tch.sqrt(1-alpha_bar)*noise_predictor(images,labels,times)) +noise \n",
    "        images = normalize_data(images)\n",
    "        i = i-1\n",
    "    return images.detach().numpy()\n",
    "    # return images.detach().numpy().squeeze(1)\n",
    "\n",
    "img = generate_image(1, tch.tensor([1]),modifiedUnet).detach().numpy().squeeze(0)\n",
    "plt.imshow(img[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
